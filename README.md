# NN-From-Scratch

An implementation of neural network components from scratch, including activation functions, layers, and other essential components like optimizers and loss functions. This project provides a deeper understanding of how each part of a neural network operates under the hood.

## Overview

This repository contains Python implementations of various neural network components from scratch. It serves as a learning resource and a practical implementation guide for building neural networks without relying on high-level frameworks.

## Components Implemented

### Activation Functions
- **ReLU (Rectified Linear Unit)** - `relu.py`
- **Sigmoid** - `sigmoid.py`
- **Softmax** - `softmax.py`
- 

### Layers
- **Linear (Fully Connected Layer)** - `linear.py`
- **Batch Normalization** - `batchnorm.py`
- **Max Pooling** - `maxpool.py`

### Models
- **Multilayer Perceptron (MLP)** - `MLP.py`
- **Convolutional Neural Network (CNN)** - `cnn.py`

### Others
- **Optimizers** - `optimizers.py`
- **Cross-Entropy Loss** - `celoss.py`


